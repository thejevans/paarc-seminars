{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review from the previous lecture\n",
    "In yesterday's Lecture 2, you learned how to use the `numpy` module, how to make your own functions, and how to import and export data. Below is a quick review before we move on to Lecture 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, to use the `numpy` module, first it must be imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do a lot with the `numpy` module. Below is an example to jog your memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0,10,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember loops? Let's use a `while` loop to make an array of 10 numbers. Let's have each element be increased by 2 compared with the previous element. Let's also have the first element of the array be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "\n",
    "# start by defining the length of the array\n",
    "arrayLength = 10\n",
    "\n",
    "# make a numpy array of 10 zeros\n",
    "\n",
    "# let's define the first element of the array\n",
    "\n",
    "# make your while loop (don't forget to initialize i!) and print out the new array \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's your quick review of `numpy` and a while loop. Now we can move on to the content of Lecture 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Data Introspection\n",
    "One of the most common tasks in experimental physics is trying to model experimental data with a function. This lecture will walk through how to accomplish this with python. First, let's import some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data was generated using the methods described in the preview notebook. Feel free to read through the preview to see how it was done.\n",
    "data_filename = \"https://raw.githubusercontent.com/astroumd/GradMap/master/notebooks/Lectures2021/Lecture3/Data/photopeak.txt\"\n",
    "x_data, y_data = np.loadtxt(data_filename, usecols=(0, 1), unpack=True)\n",
    "print(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it to see what it looks like!\n",
    "\n",
    "In python there are multiple plotting libraries, but the moset common one is `matplotlib`, and that is the one we will be using today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the matplotlib package. Really, we only need the pyplot sub-package, so we import it this way. Renaming it to 'plt' is common practice.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The most basic kind of scatter plot you can plot using matplotlib is done like this:\n",
    "plt.scatter(x_data, y_data)\n",
    "\n",
    "# At the end of a cell where you are plotting things, this line tells python that you want do display the plots you defined in the cell.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, you will want to have some theory-based motivation for the function you choose to model some set of data, but for this example, we don't know anything about the data other than the points themselves. In this type of situation, trying to fit a simple function to the data is not a bad first step in trying to understand it. What function do you think might fit this data based on how it looks in the plot?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "It looks like the data is shaped like a normal (gaussian) distribution, so let's try to fit it to that! First, let's define a gaussian function for fitting.\n",
    "    \n",
    "The equation for a Gaussian curve is the following:\n",
    "\n",
    "$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation. We also want to be able to scale our function to fit the scale of the data, so we should multiply the equation by some amplitude, A.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the body of this function so that is returns the value at x of the gaussian defined by the parameters.\n",
    "def gauss(x, mean, std, amp):\n",
    "    \"\"\"Write what the function does here!\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What values of these parameters do you think will match the data above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = \n",
    "sigma = \n",
    "amp = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try plotting the function the function with these parameters next to the data! For this, we should define some evenly-spaced x-values to calculate the function at using `np.linspace`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points = # Use np.linspace here.\n",
    "\n",
    "plt.plot(x_data, y_data, linestyle='', marker='.', markersize=12) # This line makes the same plot as plt.scatter, but avoids some quirks in matplotlib.\n",
    "# Try replacing this with plt.scatter(x_data, y_data) and see what happens!\n",
    "# plt.scatter(x_data, y_data)\n",
    "\n",
    "y_gauss = # Finish this line to get the y-values using the function you made above.\n",
    "plt.plot(x_points, y_gauss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plt.plot` is the standard do-it-all plotting function in `matplotlib`. Everything about how the series looks can be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Goodness-of-fit\n",
    "\n",
    "How good was your guess? How do you even answer that question?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "Let's use something called the $L_2$ norm: $||y - f(x)||^2$ to get a metric of the difference between our data and our function. This may sound and look fancy, but all it's doing is calculating the distance at each x-value in the data between the data y-value and the function y-value. Then, it squares those distances and adds them all together.\n",
    "\n",
    "This is defined by `np.sum((y_data - y_data_gauss)**2)` for our setup.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_gauss = # Finish this line to get y_gauss at the x_data.\n",
    "l2_norm = # Finish this line to calculate the L_2 norm.\n",
    "print(l2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the parameters to something bad and see what happens to the value of `l2_norm`. Since this definition of the $L_2$ norm is not normalized by something like a standard deviation of the data, it can't tell us in absolute terms how good our funciton fits, but it can at least tell us if one set of parameters fits better than another. This is really helpful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = \n",
    "sigma = \n",
    "amp = \n",
    "\n",
    "l2_norm = np.sum((y_data - gauss(x_data, mu, sigma, amp))**2)\n",
    "print(l2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, how do we know when we have a best fit? How would you try to figure it out?\n",
    "\n",
    "Thankfully, we don't have to create our own method to do this. The smart people working on the `scipy` package have already built an optimized tool for us to use! It's called the `curve_fit` function as is part of the `scipy.optimize` sub-package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scipy.optimize.curve_fit` is a type of minimization function. In this case, the function finds the parameters of another given function that minimize the $L_2$ norm between the data points and what our gauss function thinks the data points should be at a given x.\n",
    "\n",
    "A quick, useful way to see what a function does without having to google it is to use the built-in python `help` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(scipy.optimize.curve_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gave us a lot of information about the `curve_fit` function! As you can see, `curve_fit` takes a function as its first parameter, and it tells us exactly how to arrange the parameters of that function (thankfully, our `gauss` function should already have this form). The next two parameters `curve_fit` takes are `xdata` and `ydata` (`x_data` and `y_data` as we defined them). The rest are optional and will be talked about briefly at the end of this lecture.\n",
    "\n",
    "Let's try calling help on the `gauss` function we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think the `help` function does?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "You can see help just returns the function \"signature\" and the string at the start of the function (called the \"docstring\").\n",
    "</details>\n",
    "\n",
    "Now, let's use `curve_fit` to fit the data to our function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable names popt and pcov come from the curve_fit function. We will get into what they mean soon!\n",
    "popt, pcov = scipy.optimize.curve_fit(gauss, x_data, y_data)\n",
    "\n",
    "print(popt)\n",
    "print(pcov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you know what `popt` is? How would you find out?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "If you look back at the `help` output from `curve_fit`, `popt` is a list of the best-fit parameters of our gauss function for this data. The parameters in the list are in the order that the parameters are listed in our `gauss` function (`mean`, `std`, `amp`). Let's try plotting the data, our guess, and the best fit from `curve_fit`!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points = \n",
    "plt.plot(x_data, y_data, linestyle='', marker='.', markersize=12, label='data')  # The parameter \"label\" gives a name to the data series.\n",
    "\n",
    "# Try adding labels to these plots!\n",
    "plt.plot() # Finish this line to plot your hand-picked best-guess parameters from earlier.\n",
    "plt.plot()  # Finish this line to plot the best fit parameters from curve_fit.\n",
    "\n",
    "# This function produces a legend of the plots and their labels. You can leave this line as is.\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close was your guess to the best fit?\n",
    "\n",
    "\n",
    "## D. Interpreting Fitting Errors\n",
    "`pcov` is a little more complicated. `pcov` is what's called the \"covariance matrix\" of the best fit parameters. As shown in the `help` output, the standard deviations of the parameters can be recovered from this matrix in the following way: `perr = np.sqrt(np.diag(pcov))`\n",
    "\n",
    "Since we aren't teaching linear algebra here, all of the matrix manipulations will be given to you. All that needs to be taken away from this is how to read this specific matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perr = np.sqrt(np.diag(pcov)) # This comes straight from the curve_fit help output.\n",
    "\n",
    "# This is an example of string formatting in python.\n",
    "# Each set of {} corresponds to a parameter passed to .format().\n",
    "# .3e means \"format this number in scientific notation with 3 digits after the decimal\".\n",
    "# using \"*perr\" is a nice python trick that expands perr into the three individual values it contains.\n",
    "# All of this is out of the scope of this lecture, but it's good to get exposure to these things.\n",
    "# Just look at how nice the out put looks!\n",
    "print('s_mu = {:.3e}, s_sigma = {:.3e}, s_amp = {:.3e}'.format(*perr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix looks like this for the parameters in our `gauss` function\n",
    "\n",
    "\\begin{bmatrix}\n",
    "s_{\\mu}^2 & cov(\\mu, \\sigma) & cov(\\mu, A)\\\\\n",
    "cov(\\sigma, \\mu) & s_{\\sigma}^2 & cov(\\sigma, A)\\\\\n",
    "cov(A, \\mu) & cov(A, \\sigma) & s_A^2\n",
    "\\end{bmatrix}\n",
    "\n",
    "Where $s_x^2$ is the variance of a parameter $x$ and $s_x$ is its estimated standard deviation, and $cov(x, y)$ is the covariance between parameters $x$ and $y$.\n",
    "\n",
    "Can you guess what `np.sqrt(np.diag(pcov)` does now?\n",
    "\n",
    "Covariance can be difficult to visualize. It's often much easier to look at something called the \"correlation coefficient\" instead. The correlation coefficients can be easily found from the covariance matrix by using this transformation:\n",
    "\n",
    "$cor(x, y) = \\frac{cov(x, y)}{s_x s_y}$\n",
    "\n",
    "Performing this transformation on the covariance matrix gives the correlation matrix, which looks like this:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 & cor(\\mu, \\sigma) & cor(\\mu, A)\\\\\n",
    "cor(\\sigma, \\mu) & 1 & cor(\\sigma, A)\\\\\n",
    "cor(A, \\mu) & cor(A, \\sigma) & 1\n",
    "\\end{bmatrix}\n",
    "\n",
    "The code for this is a bit beyond the scope of this lecture, so it has been done for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perr_transpose = np.atleast_2d(perr).T\n",
    "pcor = pcov / perr / perr_transpose\n",
    "print(pcor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficients say how related the parameters are (-1 = perfectly anti-correlated, 0 = perfectly independent, 1 = perfectly correlated). As you can see here, the mean is independent from both of the other parameters, but the standard deviation and the amplitude are fairly anti-correlated. Any guesses on why this would be?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "As you increase the amplitude, the plot stretches up, moving the curve away from many points in the bell curve. This can be compensated for by reducing the standard deviation, bringing the sides of the bell curve in toward the points. The converse is also true.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of minimization that curve_fit does is called a least-squares fit. To do this least-squares fit, the function uses a numerical method called \"lm\" by default. It's generally a good idea to try to fit using the defaults. Sometimes, this won't work, and you will need to take more steps. The procedure is generally as follows:\n",
    "\n",
    "1. Add bounds and initial guesses to the parameters so that the minimizer knows where to look for the best-fit.\n",
    "1. Add a Jacobian matrix to tell the minimizer how quickly to change the parameters relative to each other.\n",
    "1. Try different metrics to determine goodness-of-fit and/or different minimizers.\n",
    "1. Switch to some form of deterministic machine learning algorithm.\n",
    "1. Switch to a non-deterministic machine learning algorithm.\n",
    "\n",
    "These steps get progressively more complicated and time consuming. Between every step you should be asking yourself whether or not there isn't some better way to formulate your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge problems:\n",
    "\n",
    "insert challenge exercises using p0/bounds and jac here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
