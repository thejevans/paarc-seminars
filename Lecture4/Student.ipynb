{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review from the previous seminars\n",
    "In Seminar 3, we learned how to use the `numpy` module, how to make our own functions, and how to import and export data. Below is a quick review before we move on to Seminar 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, to use the `numpy` module, first it must be imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do a lot with the `numpy` module. Below is an example to jog your memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0,10,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember loops? Here are some examples of `while` and `for` loops to make an array of 10 numbers. Let's have each element be increased by 2 compared with the previous element. Let's also have the first element of the array be 1.\n",
    "\n",
    "`while` loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's intialize the array as empty (garbage data)\n",
    "my_array = np.empty(10)\n",
    "\n",
    "# next, let's define the first element of the array\n",
    "my_array[0] = 1\n",
    "\n",
    "i = 1  # with the first element defined, we can calculate the rest of the sequence beginning with the 2nd element\n",
    "while i < len(my_array):\n",
    "    my_array[i] = my_array[i - 1] + 2\n",
    "    i += 1  # the same as i = i + 1\n",
    "\n",
    "# this is called an 'f-string'. it's a very compact way to format text\n",
    "print(f'while loop: {my_array = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`for` loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array2 = np.empty(10)\n",
    "\n",
    "# noticing a pattern in our array, we can use the same operation on every item to generate the same array\n",
    "# without having to reference preivous items.\n",
    "for i in range(len(my_array2)):\n",
    "    my_array2[i] = i * 2 + 1\n",
    "    \n",
    "print(f'for loop: {my_array2 = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus single line options!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a 'list comprehension' to generate a list, then turn it into an array\n",
    "# 'list comprehesions' are special for loops that are slightly faster to compute and to type!\n",
    "my_array3 = np.array([i * 2 + 1 for i in range(10)])\n",
    "print(f'list comp: {my_array3 = }')\n",
    "\n",
    "# use the math from above directly on a generated array. even faster!\n",
    "my_array4 = np.arange(10) * 2 + 1\n",
    "print(f'direct:    {my_array4 = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's your quick review of `numpy` and loops. Now we can move on to the content of Seminar 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Data Exploration\n",
    "One of the most common tasks in experimental physics is trying to model experimental data with a function. This lecture will walk through how to accomplish this with python. First, let's import some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = \"https://raw.githubusercontent.com/astroumd/GradMap/master/notebooks/Lectures2021/Lecture3/Data/photopeak.txt\"\n",
    "x_data, y_data = np.loadtxt(data_filename, usecols=(0, 1), unpack=True)\n",
    "print(f'{x_data = }')\n",
    "print(f'{y_data = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good idea to play around with data to get a feel for it. Let's try a few things!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy has some functions built in that can give you some idea of what you're looking at\n",
    "\n",
    "# np.mean: mean value of elements in an array\n",
    "print(f'{np.mean(x_data) = }')\n",
    "print(f'{np.mean(y_data) = }\\n')\n",
    "\n",
    "# np.std: standard deviation of elements in array\n",
    "#print(f'{np.std(x_data) = }')\n",
    "#print(f'{np.std(y_data) = }\\n')\n",
    "\n",
    "# np.diff: difference between adjacent elements in an array\n",
    "#print(f'{np.diff(x_data) = }')\n",
    "#print(f'{np.diff(y_data) = }\\n')\n",
    "\n",
    "# np.argsort: array of indicies in sorted order for an array\n",
    "#sorted_indicies = np.argsort(x_data)\n",
    "#print(f'{sorted_indicies = }\\n')\n",
    "\n",
    "# diff now with sorted indicies\n",
    "#print(f'{np.diff(x_data[sorted_indicies]) = }')\n",
    "#print(f'{np.diff(y_data[sorted_indicies]) = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it to see what it looks like!\n",
    "\n",
    "Quick review of Seminar 2: In python there are multiple plotting libraries, but the moset common one is `matplotlib`, and that is the one we will be using today. The plots in Seminar 2 were primarily for presenting information to others. The plots we will be making today will be primarily for exploration and for sanity checks, so we don't need to take the time to make them pretty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the matplotlib package. Really, we only need the pyplot sub-package, so we import it this way.\n",
    "# Renaming it to 'plt' is common practice.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# To start, lets histogram the data on both axes. This is often a useful tool to get more visual information\n",
    "# For the x-axis:\n",
    "plt.hist(x_data)\n",
    "\n",
    "# When you are ready to produce a plot after building it,\n",
    "# this line tells python that you want do display the plot you defined.\n",
    "plt.show()\n",
    "\n",
    "# And the y-axis:\n",
    "plt.hist(y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots are useful when you have two-dimensional data like we have here.\n",
    "# The most basic kind of scatter plot you can plot using matplotlib is done like this:\n",
    "plt.scatter(x_data, y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Fitting Data\n",
    "\n",
    "Usually, you will want to have some theory-based motivation for the function you choose to model some set of data, but for this example, we don't know anything about the data other than the points themselves. In this type of situation, trying to fit a simple function to the data is not a bad first step in trying to understand it. What function do you think might fit this data based on how it looks in the plot?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "It looks like the data is shaped like a normal (gaussian) distribution, so let's try to fit it to that! First, let's define a gaussian function for fitting.\n",
    "    \n",
    "The equation for a Gaussian curve is the following:\n",
    "\n",
    "$f(x) = Ae^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation. We also want to be able to scale our function to fit the scale of the data, so we multiply the equation by some amplitude, A.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the body of this function so that is returns the value at x of the gaussian defined by the parameters.\n",
    "def gauss(x, mean, std, amp):\n",
    "    \"\"\"Write what the function does here!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_data, y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What values of these parameters do you think will match the data above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = \n",
    "sigma = \n",
    "amp = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try plotting the function the function with these parameters next to the data! For this, we should define some evenly-spaced x-values to calculate the function at using `np.linspace`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points = # Use np.linspace here.\n",
    "\n",
    "plt.plot(x_data, y_data, linestyle='', marker='.', markersize=12) # This line makes the same plot as plt.scatter, but avoids some quirks in matplotlib.\n",
    "# Try replacing this with plt.scatter(x_data, y_data) and see what happens!\n",
    "# plt.scatter(x_data, y_data)\n",
    "\n",
    "y_gauss = # Finish this line to get the y-values using the function you made above.\n",
    "plt.plot(x_points, y_gauss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plt.plot` is the standard do-it-all plotting function in `matplotlib`. Everything about how the series looks can be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Goodness-of-fit\n",
    "\n",
    "How good was your guess? How do you even answer that question?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "Let's use something called the \"sum of squared residuals\" ($\\sum_{i = 0}^n ||y_i - f(x_i)||^2$) to get a metric of the difference between our data and our function. This may sound and look fancy, but all it's doing is calculating the distance at each x-value in the data between the data y-value and the function y-value. Then, it squares those distances and adds them all together.\n",
    "\n",
    "This is defined by `np.sum((y_data - y_data_gauss)**2)` for our setup.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_gauss = # Finish this line to get y_gauss at the x_data.\n",
    "res_sum = # Finish this line to calculate the L_2 norm.\n",
    "print(f'{res_sum = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the parameters to something bad and see what happens to the value of `res_sum`. Since this metric is not normalized by something like a standard deviation of the data, it can't tell us in absolute terms how good our funciton fits, but it can at least tell us if one set of parameters fits better than another. This is really helpful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = \n",
    "sigma = \n",
    "amp = \n",
    "\n",
    "res_sum = np.sum((y_data - gauss(x_data, mu, sigma, amp))**2)\n",
    "print(f'{res_sum = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, how do we know when we have a best fit? How would you try to figure it out?\n",
    "\n",
    "Thankfully, we don't have to create our own method to do this. The smart people working on the `scipy` package have already built an optimized tool for us to use! It's called the `curve_fit` function as is part of the `scipy.optimize` sub-package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scipy.optimize.curve_fit` is a type of minimization function. In this case, the function finds the parameters of our `gauss` function (`mean`, `std`, `amp`) that minimize the sum of squared residuals between the data points and what our gaussian function thinks the data points should be at a given x.\n",
    "\n",
    "A quick, useful way to see what a function does without having to google it is to use the built-in python `help` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(scipy.optimize.curve_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gave us a lot of information about the `curve_fit` function! As you can see, `curve_fit` takes a function as its first parameter, and it tells us exactly how to arrange the parameters of that function (thankfully, our `gauss` function should already have this form). The next two parameters `curve_fit` takes are `xdata` and `ydata` (`x_data` and `y_data` as we defined them). The rest are optional and will be talked about briefly at the end of this seminar.\n",
    "\n",
    "Let's try calling help on the `gauss` function we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think the `help` function does?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "You can see help just returns the function \"signature\" and the string at the start of the function (called the \"docstring\").\n",
    "</details>\n",
    "\n",
    "Now, let's use `curve_fit` to fit the data to our function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable names popt and pcov come from the curve_fit function. We will get into what they mean soon!\n",
    "popt, pcov = scipy.optimize.curve_fit(gauss, x_data, y_data)\n",
    "\n",
    "print(popt)\n",
    "print(pcov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you know what `popt` is? How would you find out?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "If you look back at the `help` output from `curve_fit`, `popt` is a list of the best-fit parameters of our gauss function for this data. The parameters in the list are in the order that the parameters are listed in our `gauss` function (`mean`, `std`, `amp`). Let's try plotting the data, our guess, and the best fit from `curve_fit`!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points = \n",
    "plt.plot(x_data, y_data, linestyle='', marker='.', markersize=12, label='data')  # The parameter \"label\" gives a name to the data series.\n",
    "\n",
    "# Try adding labels to these plots!\n",
    "plt.plot() # Finish this line to plot your hand-picked best-guess parameters from earlier.\n",
    "plt.plot()  # Finish this line to plot the best fit parameters from curve_fit.\n",
    "\n",
    "# This function produces a legend of the plots and their labels. You can leave this line as is.\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close was your guess to the best fit?\n",
    "\n",
    "## E. Interpreting Fitting Errors\n",
    "`pcov` is a little more complicated. `pcov` is what's called the \"covariance matrix\" of the best fit parameters. As shown in the `help` output, the standard deviations of the parameters can be recovered from this matrix in the following way: `perr = np.sqrt(np.diag(pcov))`\n",
    "\n",
    "Since we aren't teaching linear algebra here, all of the matrix manipulations will be given to you. All that needs to be taken away from this is how to read this specific matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perr = np.sqrt(np.diag(pcov)) # This comes straight from the curve_fit help output.\n",
    "\n",
    "# This is an example of some more complicated string formatting in python.\n",
    "# Each set of {} corresponds to a parameter passed to .format().\n",
    "# .3e means \"format this number in scientific notation with 3 digits after the decimal\".\n",
    "# using \"*perr\" is a nice python trick that expands perr into the three individual values it contains.\n",
    "# All of this is out of the scope of this lecture, but it's good to get exposure to these things.\n",
    "# Just look at how nice the out put looks!\n",
    "print('s_mu = {:.3e}, s_sigma = {:.3e}, s_amp = {:.3e}'.format(*perr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix looks like this for the parameters in our `gauss` function\n",
    "\n",
    "\\begin{bmatrix}\n",
    "s_{\\mu}^2 & cov(\\mu, \\sigma) & cov(\\mu, A)\\\\\n",
    "cov(\\sigma, \\mu) & s_{\\sigma}^2 & cov(\\sigma, A)\\\\\n",
    "cov(A, \\mu) & cov(A, \\sigma) & s_A^2\n",
    "\\end{bmatrix}\n",
    "\n",
    "Where $s_x^2$ is the variance of a parameter $x$ and $s_x$ is its estimated standard deviation, and $cov(x, y)$ is the covariance between parameters $x$ and $y$.\n",
    "\n",
    "Can you guess what `np.sqrt(np.diag(pcov))` does now?\n",
    "\n",
    "Covariance can be difficult to visualize. It's often much easier to look at something called the \"correlation coefficient\" instead. The correlation coefficients can be easily found from the covariance matrix by using this transformation:\n",
    "\n",
    "$cor(x, y) = \\frac{cov(x, y)}{s_x s_y}$\n",
    "\n",
    "Performing this transformation on the covariance matrix gives the correlation matrix, which looks like this:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 & cor(\\mu, \\sigma) & cor(\\mu, A)\\\\\n",
    "cor(\\sigma, \\mu) & 1 & cor(\\sigma, A)\\\\\n",
    "cor(A, \\mu) & cor(A, \\sigma) & 1\n",
    "\\end{bmatrix}\n",
    "\n",
    "The code for this is a bit beyond the scope of this lecture, so it has been done for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perr_transpose = np.atleast_2d(perr).T\n",
    "pcor = pcov / perr / perr_transpose\n",
    "print(pcor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficients say how related the parameters are (-1 = perfectly anti-correlated, 0 = perfectly uncorrelated, 1 = perfectly correlated). As you can see here, the mean is uncorrelated from both of the other parameters, but the standard deviation and the amplitude are fairly anti-correlated. Any guesses on why this would be?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "As you increase the amplitude, the plot stretches up, moving the curve away from many points in the bell curve. This can be compensated for by reducing the standard deviation, bringing the sides of the bell curve in toward the points. The converse is also true.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of minimization that `curve_fit` does is called a least-squares fit. To do this least-squares fit, the function uses a numerical method called \"lm\" by default. It's generally a good idea to try to fit using the defaults. Sometimes, this won't work, and you will need to take more steps. The procedure is generally as follows:\n",
    "\n",
    "1. Add bounds and initial guesses to the parameters so that the minimizer knows where to look for the best-fit.\n",
    "1. Add a Jacobian matrix to tell the minimizer how quickly to change the parameters relative to each other.\n",
    "1. Try different metrics to determine goodness-of-fit and/or different minimizers.\n",
    "1. Switch to some form of deterministic machine learning algorithm.\n",
    "1. Switch to a non-deterministic machine learning algorithm.\n",
    "\n",
    "These steps get progressively more complicated and time consuming. Between every step you should be asking yourself whether or not there isn't some better way to formulate your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
